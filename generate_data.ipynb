{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9903c8d8-0e63-413b-8ede-0b80f3c48929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334ecb71-ea19-4fdb-9fe9-cd8e0aff35a7",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc1179cd-6f4d-4e48-97d0-508bab74372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load input spike data\n",
    "mat = scipy.io.loadmat(Path('data') / 'raw' / '01.mat') # animal 01\n",
    "\n",
    "spike_train_all = mat['resp_train'] # spike train of all neurons, neurons x image x trials x milliseconds\n",
    "\n",
    "images_all = mat['images'].squeeze()\n",
    "images_all = torch.stack([torch.tensor(entry) for entry in images_all])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d09cef99-a9b0-41ed-bc86-f95c68a1dcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep well-centered channels\n",
    "indcent = mat['INDCENT'].squeeze()\n",
    "spike_train_cent = torch.tensor(spike_train_all[indcent == 1]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea514147-a6ee-4a7e-9c2c-e8a48dbf5185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indices of all small natural images\n",
    "idx_small_nat_images = torch.zeros(spike_train_all.shape[1], dtype=torch.bool)\n",
    "idx_small_nat_images[:539:2] = 1\n",
    "\n",
    "# get indices of all big natural images\n",
    "idx_big_nat_images = torch.ones(spike_train_all.shape[1], dtype=torch.bool)\n",
    "idx_big_nat_images[:539:2] = 0\n",
    "idx_big_nat_images[540:] = 0\n",
    "\n",
    "# get indices of all gratings\n",
    "idx_gratings = torch.zeros(spike_train_all.shape[1], dtype=torch.bool)\n",
    "idx_gratings[540:] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfc47e5f-eda7-4477-876c-8cefa242110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use gratings\n",
    "spike_train_cent = spike_train_cent[:, idx_gratings, :, :]\n",
    "images_all = images_all[idx_gratings, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8a0442-aa8a-480f-af05-868eab2c701d",
   "metadata": {},
   "source": [
    "### Generate train and test data (only for gratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "717488d5-9fff-4089-9e71-194631fc94fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = len(images_all)\n",
    "train_frac = 0.8\n",
    "val_frac = 0.1\n",
    "test_frac = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e683584-2886-432b-8fe7-3478d774f9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate shuffled indices\n",
    "indices = np.arange(n_images)\n",
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e97d0758-99f4-473c-a0c7-9d050e929953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute split sizes\n",
    "train_size = int(train_frac * n_images)\n",
    "val_size = int(val_frac * n_images)\n",
    "test_size = int(test_frac * n_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6be6accd-d838-4bd3-bccd-04f81d55fddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split indices\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:train_size + val_size]\n",
    "test_indices = indices[train_size + val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa51d45f-18a9-42c5-bbac-f70ddf17def8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create boolean masks\n",
    "train_mask = np.zeros(n_images, dtype=bool)\n",
    "val_mask = np.zeros(n_images, dtype=bool)\n",
    "test_mask = np.zeros(n_images, dtype=bool)\n",
    "\n",
    "train_mask[train_indices] = True\n",
    "val_mask[val_indices] = True\n",
    "test_mask[test_indices] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3616b19b-b9e2-46e3-a277-ac1886058a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training, validation, and test sets\n",
    "train_images = images_all[train_mask, :, :]\n",
    "val_images = images_all[val_mask, :, :]\n",
    "test_images = images_all[test_mask, :, :]\n",
    "\n",
    "train_spikes = spike_train_cent[:, train_mask, :, :]\n",
    "val_spikes = spike_train_cent[:, val_mask, :, :]\n",
    "test_spikes = spike_train_cent[:, test_mask, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "173100c0-2203-4a43-a6cf-21c266b9ea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save files\n",
    "save_dir = Path('data')\n",
    "\n",
    "torch.save(train_images, Path(save_dir) / 'train_images_gratings.pt')\n",
    "torch.save(val_images, Path(save_dir) / 'val_images_gratings.pt')\n",
    "torch.save(test_images, Path(save_dir) / 'test_images_gratings.pt')\n",
    "\n",
    "torch.save(train_spikes, Path(save_dir) / 'train_spikes_gratings.pt')\n",
    "torch.save(val_spikes, Path(save_dir) / 'val_spikes_gratings.pt')\n",
    "torch.save(test_spikes, Path(save_dir) / 'test_spikes_gratings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d3f3a5-b47d-4fe1-a761-5d1832661218",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
